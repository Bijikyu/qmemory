/**
 * Database Operation Helper Utilities
 * 
 * Purpose: Centralizes database operation patterns that are duplicated across
 * multiple service files, particularly error handling, connection management,
 * and common query patterns used in payment processing.
 * 
 * Consolidation rationale:
 * - MongoDB error handling (duplicate key errors, connection issues) repeated across services
 * - Query pattern optimizations duplicated in multiple model interactions
 * - Transaction handling logic scattered across payment and subscription services
 * - Standardizing these patterns improves reliability and maintainability
 * 
 * Usage:
 * - Import specific helper functions in service files and controllers
 * - Use for consistent database error handling and query optimization
 * - Reduces code duplication and ensures uniform database interactions
 */

const { logger } = require('./logger');
const { qerrors } = require('./offlineMode');

/**
 * MongoDB Error Handler
 * 
 * Purpose: Centralizes MongoDB error handling logic duplicated across services
 * Consolidates error handling patterns from paymentService.js and other data services.
 * 
 * Error handling rationale:
 * - Duplicate key errors (11000) often indicate race conditions or retry scenarios
 * - Connection errors require different handling than validation errors
 * - Consistent error classification enables appropriate client responses
 * - Structured error logging supports debugging and monitoring
 */
function handleMongoError(error, operation, context = {}) {
    const errorInfo = {
        operation,
        context,
        timestamp: new Date().toISOString(),
        errorCode: error.code,
        errorMessage: error.message
    };

    // Handle duplicate key errors (common in payment processing)
    if (error.code === 11000) {
        const duplicateKeyInfo = {
            ...errorInfo,
            type: 'DUPLICATE_KEY_ERROR',
            severity: 'medium',
            recoverable: true,
            keyValue: error.keyValue || 'unknown'
        };
        
        logger.warn('Duplicate key error in database operation', duplicateKeyInfo);
        qerrors(error, `Duplicate key error in ${operation}`, context);
        
        return {
            type: 'DUPLICATE_KEY_ERROR',
            message: 'Resource already exists',
            recoverable: true,
            statusCode: 409
        };
    }

    // Handle validation errors
    if (error.name === 'ValidationError') {
        const validationInfo = {
            ...errorInfo,
            type: 'VALIDATION_ERROR',
            severity: 'low',
            recoverable: true,
            validationErrors: Object.keys(error.errors || {})
        };
        
        logger.warn('Database validation error', validationInfo);
        qerrors(error, `Validation error in ${operation}`, context);
        
        return {
            type: 'VALIDATION_ERROR',
            message: 'Data validation failed',
            details: error.errors,
            recoverable: true,
            statusCode: 400
        };
    }

    // Handle connection errors
    if (error.name === 'MongoNetworkError' || error.name === 'MongoServerError') {
        const connectionInfo = {
            ...errorInfo,
            type: 'CONNECTION_ERROR',
            severity: 'high',
            recoverable: false
        };
        
        logger.error('Database connection error', connectionInfo);
        qerrors(error, `Connection error in ${operation}`, context);
        
        return {
            type: 'CONNECTION_ERROR',
            message: 'Database connection failed',
            recoverable: false,
            statusCode: 503
        };
    }

    // Handle timeout errors
    if (error.name === 'MongoServerSelectionError' || error.message.includes('timeout')) {
        const timeoutInfo = {
            ...errorInfo,
            type: 'TIMEOUT_ERROR',
            severity: 'high',
            recoverable: true
        };
        
        logger.error('Database timeout error', timeoutInfo);
        qerrors(error, `Timeout error in ${operation}`, context);
        
        return {
            type: 'TIMEOUT_ERROR',
            message: 'Database operation timed out',
            recoverable: true,
            statusCode: 504
        };
    }

    // Handle unknown errors
    const unknownInfo = {
        ...errorInfo,
        type: 'UNKNOWN_ERROR',
        severity: 'high',
        recoverable: false
    };
    
    logger.error('Unknown database error', unknownInfo);
    qerrors(error, `Unknown error in ${operation}`, context);
    
    return {
        type: 'UNKNOWN_ERROR',
        message: 'Database operation failed',
        recoverable: false,
        statusCode: 500
    };
}

/**
 * Safe Database Operation Wrapper
 * 
 * Purpose: Wraps database operations with consistent error handling and logging
 * Consolidates try-catch patterns from multiple service files.
 * 
 * Wrapper rationale:
 * - Consistent error handling across all database operations
 * - Automatic logging and monitoring for database interactions
 * - Retry logic for recoverable errors
 * - Performance tracking for optimization
 */
async function safeDbOperation(operation, operationName, context = {}) {
    const startTime = Date.now();
    
    try {
        const result = await operation();
        const processingTime = Date.now() - startTime;
        
        logger.debug('Database operation completed', {
            operation: operationName,
            processingTime,
            context,
            success: true
        });
        
        return { success: true, data: result, processingTime };
        
    } catch (error) {
        const processingTime = Date.now() - startTime;
        const errorResult = handleMongoError(error, operationName, context);
        
        logger.error('Database operation failed', {
            operation: operationName,
            processingTime,
            context,
            error: errorResult
        });
        
        return { 
            success: false, 
            error: errorResult, 
            processingTime 
        };
    }
}

/**
 * Retry Logic for Database Operations
 * 
 * Purpose: Implements retry logic for recoverable database errors
 * Consolidates retry patterns from webhook processing and payment services.
 * 
 * Retry rationale:
 * - Network issues and timeouts are often transient
 * - Duplicate key errors in race conditions may resolve with retry
 * - Exponential backoff prevents overwhelming database during issues
 * - Maximum retry limits prevent infinite loops
 */
async function retryDbOperation(operation, operationName, options = {}) {
    const {
        maxRetries = 3,
        baseDelay = 1000,
        maxDelay = 10000,
        retryCondition = (error) => error.recoverable,
        context = {}
    } = options;

    let lastError;
    
    for (let attempt = 1; attempt <= maxRetries + 1; attempt++) {
        const result = await safeDbOperation(operation, operationName, { ...context, attempt });
        
        if (result.success) {
            if (attempt > 1) {
                logger.info('Database operation succeeded after retry', {
                    operation: operationName,
                    attempt,
                    context
                });
            }
            return result;
        }
        
        lastError = result.error;
        
        // Don't retry if not recoverable or max attempts reached
        if (!retryCondition(result.error) || attempt > maxRetries) {
            break;
        }
        
        // Calculate delay with exponential backoff
        const delay = Math.min(baseDelay * Math.pow(2, attempt - 1), maxDelay);
        
        logger.warn('Database operation failed, retrying', {
            operation: operationName,
            attempt,
            nextRetryIn: delay,
            error: result.error.type,
            context
        });
        
        await new Promise(resolve => setTimeout(resolve, delay));
    }
    
    return { success: false, error: lastError };
}

/**
 * Idempotency Helper for Database Operations
 * 
 * Purpose: Implements idempotency checks for critical operations
 * Consolidates idempotency patterns from webhook and payment processing.
 * 
 * Idempotency rationale:
 * - Payment operations must not be duplicated (financial impact)
 * - Webhook events may be delivered multiple times
 * - Race conditions can cause duplicate processing
 * - Idempotency keys prevent accidental duplicate operations
 */
async function ensureIdempotency(model, idempotencyKey, operation, operationName) {
    const context = { idempotencyKey, model: model.modelName };
    
    // Check if operation already exists
    const existingRecord = await safeDbOperation(
        () => model.findOne({ [idempotencyKey.field]: idempotencyKey.value }),
        `${operationName}_idempotency_check`,
        context
    );
    
    if (!existingRecord.success) {
        return existingRecord;
    }
    
    if (existingRecord.data) {
        logger.info('Idempotent operation detected', {
            operation: operationName,
            existingId: existingRecord.data._id,
            context
        });
        
        return {
            success: true,
            data: existingRecord.data,
            idempotent: true
        };
    }
    
    // Perform operation with duplicate key handling
    const result = await safeDbOperation(operation, operationName, context);
    
    // Handle race condition where another process created the record
    if (!result.success && result.error.type === 'DUPLICATE_KEY_ERROR') {
        logger.info('Race condition detected, fetching existing record', {
            operation: operationName,
            context
        });
        
        const raceResult = await safeDbOperation(
            () => model.findOne({ [idempotencyKey.field]: idempotencyKey.value }),
            `${operationName}_race_recovery`,
            context
        );
        
        if (raceResult.success && raceResult.data) {
            return {
                success: true,
                data: raceResult.data,
                idempotent: true,
                raceCondition: true
            };
        }
    }
    
    return result;
}

/**
 * Query Optimization Helper
 * 
 * Purpose: Provides optimized query patterns for common operations
 * Consolidates query optimization patterns from multiple services.
 * 
 * Optimization rationale:
 * - Consistent field selection reduces network overhead
 * - Proper indexing hints improve query performance
 * - Lean queries reduce memory usage for large datasets
 * - Pagination patterns prevent memory exhaustion
 */
function optimizeQuery(baseQuery, options = {}) {
    const {
        lean = true,
        select = null,
        limit = null,
        skip = null,
        sort = null,
        populate = null,
        hint = null
    } = options;

    let query = baseQuery;
    
    if (lean) {
        query = query.lean();
    }
    
    if (select) {
        query = query.select(select);
    }
    
    if (limit) {
        query = query.limit(limit);
    }
    
    if (skip) {
        query = query.skip(skip);
    }
    
    if (sort) {
        query = query.sort(sort);
    }
    
    if (populate) {
        query = query.populate(populate);
    }
    
    if (hint) {
        query = query.hint(hint);
    }
    
    return query;
}

/**
 * Aggregation Pipeline Helper
 * 
 * Purpose: Provides common aggregation patterns for analytics and reporting
 * Consolidates aggregation logic used across analytics and reporting services.
 * 
 * Pipeline rationale:
 * - Standardized pipeline stages improve performance
 * - Common patterns reduce code duplication
 * - Optimized aggregations minimize resource usage
 * - Consistent result formatting aids client processing
 */
function createAggregationPipeline(stages = []) {
    const pipeline = [];
    
    // Add common optimization stages
    stages.forEach(stage => {
        if (stage.match) {
            pipeline.push({ $match: stage.match });
        }
        
        if (stage.group) {
            pipeline.push({ $group: stage.group });
        }
        
        if (stage.sort) {
            pipeline.push({ $sort: stage.sort });
        }
        
        if (stage.limit) {
            pipeline.push({ $limit: stage.limit });
        }
        
        if (stage.project) {
            pipeline.push({ $project: stage.project });
        }
        
        if (stage.lookup) {
            pipeline.push({ $lookup: stage.lookup });
        }
        
        if (stage.unwind) {
            pipeline.push({ $unwind: stage.unwind });
        }
    });
    
    return pipeline;
}

module.exports = {
    handleMongoError,
    safeDbOperation,
    retryDbOperation,
    ensureIdempotency,
    optimizeQuery,
    createAggregationPipeline
};