@oakenshield/streaming-json-serializer
Purpose: A memory-efficient streaming JSON serializer for handling large payloads incrementally.

Explanation:
This streaming JSON serializer processes large JSON payloads incrementally to prevent memory exhaustion and improve performance. It solves the problem of loading entire large JSON objects into memory when processing or validating data. The module provides size estimation, chunked serialization, and safe parsing with memory protection. It's broadly applicable across any system that handles large JSON responses, file processing, or data streaming where memory efficiency is critical.

// Streaming JSON serializer for large payloads
// Implements incremental JSON serialization to prevent large memory allocations
interface JsonSerializeOptions {
  maxChunkSize?: number;
  indent?: number;
}
/**
 * Streams JSON serialization in chunks to prevent large memory allocations
 * @param obj - Object to serialize
 * @param options - Serialization options
 * @returns Generator that yields JSON chunks
 */
export function* streamJsonStringify(obj: any, options: JsonSerializeOptions = {}): Generator<string> {
  const maxChunkSize = options.maxChunkSize || 4096;
  const indent = options.indent || 0;
  
  // CRITICAL FIX: Add size validation before JSON.stringify to prevent large allocations
  const estimatedSize = estimateJsonSize(obj);
  if (estimatedSize > 10 * 1024 * 1024) { // 10MB limit
    throw new Error(`Object too large for streaming serialization (${Math.round(estimatedSize / (1024 * 1024))}MB)`);
  }
  
  const jsonString = JSON.stringify(obj, null, indent);
  
  // For small objects, return as single chunk
  if (jsonString.length <= maxChunkSize) {
    yield jsonString;
    return;
  }
  
  // Stream large JSON in chunks
  let pos = 0;
  while (pos < jsonString.length) {
    const end = Math.min(pos + maxChunkSize, jsonString.length);
    yield jsonString.substring(pos, end);
    pos = end;
  }
}
/**
 * Estimates JSON string size before serialization to avoid large allocations
 * @param obj - Object to estimate size for
 * @returns Estimated size in bytes
 */
export const estimateJsonSize=(obj:any):number=>{
  if(obj==null)return 4;
  if(typeof obj==="string")return obj.length+2;
  if(typeof obj==="number")return String(obj).length;
  if(typeof obj==="boolean")return obj?4:5;
  if(Array.isArray(obj))return obj.reduce((size,item)=>size+estimateJsonSize(item)+1,1);
  if(typeof obj==="object"){
    let size=2;
    for(const [key,value]of Object.entries(obj))size+=key.length+3+estimateJsonSize(value);
    return size;
  }
  return 50;
};
/**
 * Safe JSON stringify with size limits and truncation
 * @param obj - Object to serialize
 * @param maxSize - Maximum size before truncation
 * @returns Truncated JSON string
 */
export const safeJsonStringify=(obj:any,maxSize:number=500000):string=>{
  const estimatedSize=estimateJsonSize(obj);
  return estimatedSize<=maxSize?JSON.stringify(obj):JSON.stringify(truncateObjectForLogging(obj,maxSize));
};
/**
 * Truncates object for logging purposes to prevent large allocations
 * @param obj - Object to truncate
 * @param maxSize - Maximum target size
 * @returns Truncated object
 */
const truncateObjectForLogging=(obj:any,maxSize:number):any=>{
  if(obj==null)return obj;
  if(typeof obj==="string"){
    const truncateLength=Math.max(0,maxSize-7);
    return obj.length<=truncateLength?obj:obj.substring(0,truncateLength)+'...[TRUNCATED]';
  }
  if(Array.isArray(obj)){
    const truncatedArray:any[]=[];
    let currentSize=2;
    for(let i=0;i<obj.length&&currentSize<maxSize-20;i++){
      const itemSize=estimateJsonSize(obj[i]);
      if(currentSize+itemSize+1>maxSize-20){
        truncatedArray.push('...[TRUNCATED]');
        break;
      }
      truncatedArray.push(truncateObjectForLogging(obj[i],maxSize-currentSize-1));
      currentSize+=itemSize+1;
    }
    truncatedArray.length<obj.length&&truncatedArray.push(`...[${obj.length-truncatedArray.length} more items truncated]`);
    return truncatedArray;
  }
  if(typeof obj==="object"){
    const truncatedObj:Record<string,any>={};
    let currentSize=2;
    const entries=Object.entries(obj);
    for(let i=0;i<entries.length&&currentSize<maxSize-20;i++){
      const [key,value]=entries[i];
      const keySize=key.length+3;
      const valueSize=estimateJsonSize(value);
      if(currentSize+keySize+valueSize>maxSize-20){
        truncatedObj['...[TRUNCATED]']=`${entries.length-i} more properties truncated`;
        break;
      }
      truncatedObj[key]=truncateObjectForLogging(value,maxSize-currentSize-keySize);
      currentSize+=keySize+valueSize;
    }
    return truncatedObj;
  }
  return obj;
};
/**
 * Memory-efficient JSON parser for large payloads
 * Uses streaming approach when available
 */
export class StreamingJsonParser {
  private buffer: string = '';
  private maxBufferSize: number;
  
  constructor(maxBufferSize: number = 1024 * 1024) { // 1MB default
    this.maxBufferSize = maxBufferSize;
  }
  
  /**
   * Parse JSON with memory protection
   * @param chunk - JSON chunk to parse
   * @returns Parsed object or null if incomplete
   */
  parse(chunk: string): any {
    this.buffer += chunk;
    
    // Protect against buffer overflow
    if (this.buffer.length > this.maxBufferSize) {
      throw new Error(`JSON buffer exceeded maximum size of ${this.maxBufferSize} bytes`);
    }
    
    try {
      const result = JSON.parse(this.buffer);
      this.buffer = ''; // Clear buffer on successful parse
      return result;
    } catch (error) {
      // If parsing fails, we might have incomplete JSON
      if (error instanceof SyntaxError && error.message.includes('Unexpected end of JSON input')) {
        return null; // Incomplete JSON, wait for more chunks
      }
      throw error; // Re-throw other parsing errors
    }
  }
  
  /**
   * Reset parser buffer
   */
  reset(): void {
    this.buffer = '';
  }
}